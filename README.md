# TEXT2AV (Text to Audio and Video)

Welcome to the GitHub repository for TEXT2AV (Text to Audio and Video)! This project aims to develop a machine learning-based system that can convert text to audio and video as per the user's request.

## Abstract

The project focuses on utilizing machine learning techniques to convert text into audio and video formats. It integrates various technologies and models, including ModelsScope for text-to-video generation, Wave2Lip for audio-to-face synchronization, and Google TTS for text-to-audio conversion. The project is named "TEXT2AV" (Text to Audio and Video).

## Motivation

The rapid advancement of technology has increased the demand for multimedia content. People prefer consuming information in audio and video formats rather than reading lengthy texts. Text-to-speech (TTS) technology has made significant progress and finds applications in voice assistants and audiobooks. However, existing TTS systems lacked interactivity and human-like interactions. This project was motivated by the desire to create a TTS model with an avatar that can speak the text provided and synchronize its lip movements, providing a more engaging and realistic experience. Additionally, the project aimed to support multiple languages to cater to a diverse user base.

Moreover, generating high-quality videos directly from text inputs was another motivation for this project. Users often encounter videos that do not meet their specific requirements or contain unnecessary noise. By using diffusion models and GANs, this project aimed to generate high-quality videos at no cost, providing users with customized and visually appealing video content.

## Objective of the Project

The main objectives of the project are as follows:

- Develop a user-friendly platform that converts text to audio and video quickly and efficiently.
- Generate audio output using an avatar with lip sync to enhance the human-like interaction.
- Support multiple languages to cater to a diverse user base.
- Utilize diffusion models and GANs to generate high-quality videos from text inputs.
- Provide a cost-effective solution for generating high-resolution videos.

## Research Paper

A research paper titled "TEXT2AV â€“ Automated Text to Audio and Video Conversion" was published describing the details and results of this project. Here are the key details about the paper:

- Published in E3S Web Conf., Volume 430, 2023
- Presented at the 15th International Conference on Materials Processing and Characterization (ICMPC 2023)
- Article Number: 01027
- DOI: https://doi.org/10.1051/e3sconf/202343001027
- Published online: 06 October 2023

The paper provides comprehensive information on the methodology, datasets used, and results obtained from the TEXT2AV system.

## Software and Hardware Requirements

Software Requirements:
- Operating System: Windows, Linux, or macOS.
- Python: Version 3.6 or later.
- Deep Learning Framework: PyTorch, TensorFlow, or Keras.
- Libraries: Pillow, NumPy, OpenCV, etc.
- Text-to-Speech (TTS) Engines: Microsoft Azure, Amazon Polly, Google Cloud TTS, etc.
- Face and Lip Sync Software: Wave2Lip, FaceSwap, etc.

Hardware Requirements:
- CPU: An advanced multi-core CPU, such as the Intel Core i7 or higher, is recommended.
- GPU: A CUDA-enabled GPU (e.g., NVIDIA GeForce or Tesla) with at least 4GB of VRAM is recommended for faster training and inference.
- Memory: It is advised to have at least 16GB of RAM.
- Storage: Sufficient storage space is required for storing output audio files, model checkpoints, and training data.

## Requirements Engineering

The requirements engineering process for this project involved gathering, analyzing, and documenting specific requirements. These requirements included:

- Conversion of text to audio using Google TTS and an avatar with lip sync.
- Conversion of text to video using ModelsScope and diffusion models.
- Support for multiple languages in both audio and video outputs.
- Integration of Wave2Lip for audio-to-face synchronization.
- Utilization of deep learning frameworks such as PyTorch, TensorFlow, or Keras.
- Utilization of libraries such as Pillow, NumPy, and OpenCV for image and video processing.
- Compatibility with Windows, Linux, or macOS operating systems.
- Hardware requirements, including an advanced multi-core CPU, CUDA-enabledGPU, sufficient RAM, and storage space.

## Results

To see the results or demos of the TEXT2AV system, you can navigate to the following location:

- [https://drive.google.com/drive/folders/1ASzQxhes8sU7JlC3la28fzhia3HyVnCi?usp=sharing]

## Contributions

Contributions, suggestions, and bug reports are welcome! If you would like to contribute to this project, please follow the standard GitHub workflow for pull requests. Let's collaborate and make TEXT2AV even more impressive together!

## License

This project is licensed under the MIT License. Feel free to use, modify, and distribute the code to create your own text-to-audio and video projects.

## Acknowledgements

I would like to express my gratitude to the open-source community for their continuous contributions and the creators of the ModelsScope, Wave2Lip, and Google TTS for their exceptional technologies and models. Special thanks to all the resources and tutorials that have helped me in developing this project.

## Contact

If you have any questions or suggestions, feel free to reach out to me. You can find my contact information in the profile or the project's GitHub repository.

Thank you for visiting my GitHub repository and exploring TEXT2AV (Text to Audio and Video) project!
